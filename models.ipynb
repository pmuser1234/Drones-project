{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38657b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: librosa in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (3.10.6)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\abhishek\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from sounddevice) (2.0.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\abhishek\\appdata\\roaming\\python\\python311\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\abhishek\\appdata\\roaming\\python\\python311\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.10.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from jax->mediapipe) (0.5.3)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abhishek\\anaconda3\\envs\\drone_ai\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python sounddevice librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085bc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment ready. CUDA: True | Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "# Core packages\n",
    "import os, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"âœ… Environment ready. CUDA:\", torch.cuda.is_available(), \"| Version:\", torch.version.cuda)\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa91959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–¶ Capturing 'up' (100 samples)\n",
      "âœ… Saved 100 samples for 'up'\n",
      "\n",
      "â–¶ Capturing 'down' (100 samples)\n",
      "âœ… Saved 100 samples for 'down'\n",
      "\n",
      "â–¶ Capturing 'left' (100 samples)\n",
      "âœ… Saved 100 samples for 'left'\n",
      "\n",
      "â–¶ Capturing 'right' (100 samples)\n",
      "âœ… Saved 100 samples for 'right'\n",
      "\n",
      "â–¶ Capturing 'forward' (100 samples)\n",
      "âœ… Saved 100 samples for 'forward'\n",
      "\n",
      "â–¶ Capturing 'stop' (100 samples)\n",
      "âœ… Saved 100 samples for 'stop'\n"
     ]
    }
   ],
   "source": [
    "import cv2, mediapipe as mp, time\n",
    "GESTURES = [\"up\",\"down\",\"left\",\"right\",\"forward\",\"stop\"]\n",
    "SAVE_DIR = \"dataset/gesture_npy\"; os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "mp_hands = mp.solutions.hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "def capture(label, n=100):\n",
    "    cap=cv2.VideoCapture(0); collected=[]\n",
    "    print(f\"\\nâ–¶ Capturing '{label}' ({n} samples)\"); time.sleep(2)\n",
    "    while len(collected)<n:\n",
    "        ret, frame=cap.read(); \n",
    "        if not ret: break\n",
    "        rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        res=mp_hands.process(rgb)\n",
    "        if res.multi_hand_landmarks:\n",
    "            pts=np.array([[p.x,p.y,p.z] for p in res.multi_hand_landmarks[0].landmark]).flatten()\n",
    "            collected.append(pts)\n",
    "            cv2.putText(frame,f\"{label}:{len(collected)}/{n}\",(10,40),1,1.5,(0,255,0),2)\n",
    "        cv2.imshow(\"Gesture\",frame)\n",
    "        if cv2.waitKey(1)&0xFF==27: break\n",
    "    cap.release(); cv2.destroyAllWindows()\n",
    "    np.save(f\"{SAVE_DIR}/{label}.npy\",np.array(collected))\n",
    "    print(f\"âœ… Saved {len(collected)} samples for '{label}'\")\n",
    "\n",
    "for g in GESTURES: capture(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f8fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined gesture dataset: (600, 63) Labels: 6\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files=glob.glob(\"dataset/gesture_npy/*.npy\")\n",
    "X=[]; y=[]\n",
    "for idx,f in enumerate(files):\n",
    "    data=np.load(f); X.append(data); y.extend([idx]*len(data))\n",
    "X=np.vstack(X); y=np.array(y)\n",
    "np.save(\"dataset/gesture_landmarks.npy\",X)\n",
    "np.save(\"dataset/gesture_labels.npy\",y)\n",
    "print(\"âœ… Combined gesture dataset:\", X.shape, \"Labels:\", len(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd14e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.26G/2.26G [11:08<00:00, 3.63MB/s] \n",
      "c:\\Users\\Abhishek\\anaconda3\\envs\\drone_ai\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Voice dataset: (22971, 40)\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "WORDS=[\"up\",\"down\",\"left\",\"right\",\"go\",\"stop\"]\n",
    "SAVE=\"dataset/voice_npy\"; os.makedirs(SAVE,exist_ok=True)\n",
    "ds=SPEECHCOMMANDS(root=\"./\",download=True)\n",
    "X=[]; y=[]\n",
    "for idx,w in enumerate(WORDS):\n",
    "    for wave,sr,label,*_ in ds:\n",
    "        if label!=w: continue\n",
    "        mfcc=torchaudio.transforms.MFCC(n_mfcc=40)(wave)\n",
    "        X.append(mfcc.mean(dim=2).squeeze().numpy()); y.append(idx)\n",
    "X=np.array(X); y=np.array(y)\n",
    "np.save(f\"{SAVE}/voice_mfcc.npy\",X); np.save(f\"{SAVE}/voice_labels.npy\",y)\n",
    "print(\"âœ… Voice dataset:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f825a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Emotion dataset: (35887, 1, 48, 48) Labels: (array([0, 1]), array([26898,  8989], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fer=pd.read_csv(r\"C:\\Users\\Abhishek\\Desktop\\Abhishek\\7th sem\\Auto drones\\prj\\fer2013.csv\\fer2013.csv\")  # download from Kaggle beforehand\n",
    "X=[]; y=[]\n",
    "for _,r in fer.iterrows():\n",
    "    if r[\"emotion\"]==3: label=1\n",
    "    elif r[\"emotion\"] in [0,1,2,4,5,6]: label=0\n",
    "    else: continue\n",
    "    img=np.array(list(map(int,r[\"pixels\"].split()))).reshape(48,48)/255.0\n",
    "    X.append(img); y.append(label)\n",
    "X=np.array(X)[:,None,:,:]; y=np.array(y)\n",
    "np.save(\"dataset/faces.npy\",X); np.save(\"dataset/labels.npy\",y)\n",
    "print(\"âœ… Emotion dataset:\", X.shape, \"Labels:\", np.unique(y,return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bfd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep0: loss=1.799, acc=0.158\n",
      "Ep10: loss=1.776, acc=0.158\n",
      "Ep20: loss=1.753, acc=0.658\n",
      "Ep30: loss=1.717, acc=0.658\n",
      "Ep40: loss=1.656, acc=0.817\n",
      "Ep50: loss=1.564, acc=0.850\n",
      "Ep60: loss=1.437, acc=0.758\n",
      "Ep70: loss=1.282, acc=0.967\n",
      "âœ… gesture_model.pth saved.\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X = np.load(\"dataset/gesture_landmarks.npy\")\n",
    "y = np.load(\"dataset/gesture_labels.npy\")\n",
    "\n",
    "# Split and convert to tensors\n",
    "Xtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.2)\n",
    "Xtr, ytr = torch.tensor(Xtr, dtype=torch.float32), torch.tensor(ytr, dtype=torch.long)\n",
    "Xv,  yv  = torch.tensor(Xv,  dtype=torch.float32), torch.tensor(yv,  dtype=torch.long)\n",
    "\n",
    "# Model\n",
    "class GestureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(63, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 6)\n",
    "        )\n",
    "    def forward(self, x): return self.fc(x)\n",
    "\n",
    "m = GestureNet().cuda()\n",
    "opt = optim.Adam(m.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for e in range(80):\n",
    "    opt.zero_grad()\n",
    "    out = m(Xtr.cuda())\n",
    "    loss = crit(out, ytr.cuda())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if e % 10 == 0:\n",
    "        acc = (m(Xv.cuda()).argmax(1) == yv.cuda()).float().mean()\n",
    "        print(f\"Ep{e}: loss={loss.item():.3f}, acc={acc:.3f}\")\n",
    "\n",
    "torch.save(m.state_dict(), \"models/gesture_model.pth\")\n",
    "print(\"âœ… gesture_model.pth saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "def4ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded MFCCs: (22971, 40), Labels: (22971,), Classes: [0 1 2 3 4 5]\n",
      "Epoch 01: loss=1.4133, val_acc=0.528\n",
      "Epoch 02: loss=1.2639, val_acc=0.535\n",
      "Epoch 03: loss=1.2269, val_acc=0.556\n",
      "Epoch 04: loss=1.1984, val_acc=0.558\n",
      "Epoch 05: loss=1.1760, val_acc=0.565\n",
      "Epoch 06: loss=1.1589, val_acc=0.572\n",
      "Epoch 07: loss=1.1537, val_acc=0.578\n",
      "Epoch 08: loss=1.1373, val_acc=0.579\n",
      "Epoch 09: loss=1.1173, val_acc=0.582\n",
      "Epoch 10: loss=1.1179, val_acc=0.587\n",
      "Epoch 11: loss=1.1083, val_acc=0.579\n",
      "Epoch 12: loss=1.0970, val_acc=0.577\n",
      "Epoch 13: loss=1.0987, val_acc=0.584\n",
      "Epoch 14: loss=1.0814, val_acc=0.586\n",
      "Epoch 15: loss=1.0762, val_acc=0.585\n",
      "Epoch 16: loss=1.0718, val_acc=0.589\n",
      "Epoch 17: loss=1.0604, val_acc=0.593\n",
      "Epoch 18: loss=1.0636, val_acc=0.588\n",
      "Epoch 19: loss=1.0539, val_acc=0.594\n",
      "Epoch 20: loss=1.0508, val_acc=0.594\n",
      "Epoch 21: loss=1.0374, val_acc=0.592\n",
      "Epoch 22: loss=1.0345, val_acc=0.597\n",
      "Epoch 23: loss=1.0316, val_acc=0.598\n",
      "Epoch 24: loss=1.0247, val_acc=0.599\n",
      "Epoch 25: loss=1.0231, val_acc=0.593\n",
      "ðŸ Training complete. Best val_acc=0.599\n",
      "ðŸ“ Saved model â†’ voice_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸŽ¤ Train Voice Command Model (Fully Connected)\n",
    "# Works for dataset/voice_npy/voice_mfcc.npy (shape: N x 40)\n",
    "# ============================================================\n",
    "\n",
    "import torch, torch.nn as nn, numpy as np, os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ Load MFCC & Labels\n",
    "# ------------------------------------------------------------\n",
    "X = np.load(\"dataset/voice_npy/voice_mfcc.npy\")   # shape (N, 40)\n",
    "y = np.load(\"dataset/voice_npy/voice_labels.npy\") # shape (N,)\n",
    "\n",
    "print(f\"âœ… Loaded MFCCs: {X.shape}, Labels: {y.shape}, Classes: {np.unique(y)}\")\n",
    "\n",
    "# Normalize features sample-wise\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-6)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_dl   = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ Define Model (simple dense network)\n",
    "# ------------------------------------------------------------\n",
    "class VoiceFC(nn.Module):\n",
    "    def __init__(self, n_classes=6):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(40, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ Train Setup\n",
    "# ------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VoiceFC(n_classes=len(torch.unique(y))).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ Training Loop\n",
    "# ------------------------------------------------------------\n",
    "EPOCHS = 25\n",
    "best_acc = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); total_loss = 0\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    total_loss /= len(train_dl.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval(); correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb).argmax(1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "    acc = correct / len(X_val)\n",
    "    print(f\"Epoch {epoch+1:02d}: loss={total_loss:.4f}, val_acc={acc:.3f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"voice_model.pth\")\n",
    "\n",
    "print(f\"ðŸ Training complete. Best val_acc={best_acc:.3f}\")\n",
    "print(\"ðŸ“ Saved model â†’ voice_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcd6d819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep00: loss=0.523, acc=0.789\n",
      "Ep01: loss=0.455, acc=0.794\n",
      "Ep02: loss=0.439, acc=0.799\n",
      "Ep03: loss=0.426, acc=0.812\n",
      "Ep04: loss=0.413, acc=0.826\n",
      "Ep05: loss=0.400, acc=0.824\n",
      "Ep06: loss=0.390, acc=0.836\n",
      "Ep07: loss=0.381, acc=0.826\n",
      "Ep08: loss=0.375, acc=0.837\n",
      "Ep09: loss=0.367, acc=0.829\n",
      "Ep10: loss=0.363, acc=0.845\n",
      "Ep11: loss=0.358, acc=0.845\n",
      "Ep12: loss=0.353, acc=0.844\n",
      "Ep13: loss=0.349, acc=0.844\n",
      "Ep14: loss=0.344, acc=0.850\n",
      "âœ… emotion_model.pth saved.\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim, numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.load(\"dataset/faces.npy\")   # (N,1,48,48)\n",
    "y = np.load(\"dataset/labels.npy\")\n",
    "\n",
    "# Split\n",
    "Xtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xtr = torch.tensor(Xtr, dtype=torch.float32)\n",
    "ytr = torch.tensor(ytr, dtype=torch.long)\n",
    "Xv  = torch.tensor(Xv,  dtype=torch.float32)\n",
    "yv  = torch.tensor(yv,  dtype=torch.long)\n",
    "\n",
    "# Dataloaders  âœ…\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xv, yv), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model âœ… (pooled + fixed linear dim)\n",
    "class EmotionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               # 48â†’23\n",
    "            nn.Conv2d(8, 16, 3, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),               # 21â†’10\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 10 * 10, 2)\n",
    "        )\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "m = EmotionNet().to(device)\n",
    "opt = optim.Adam(m.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop âœ… (mini-batches)\n",
    "for e in range(15):\n",
    "    m.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = m(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation accuracy\n",
    "    m.eval(); correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            preds = m(xb.to(device)).argmax(1).cpu()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Ep{e:02d}: loss={total_loss:.3f}, acc={acc:.3f}\")\n",
    "\n",
    "torch.save(m.state_dict(), \"models/emotion_model.pth\")\n",
    "print(\"âœ… emotion_model.pth saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "682445af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_25992\\4085414792.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g=GestureNet(); g.load_state_dict(torch.load(\"models/gesture_model.pth\",map_location=\"cpu\")); g.eval()\n",
      "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_25992\\4085414792.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  v=VoiceNet();   v.load_state_dict(torch.load(\"models/voice_model.pth\",map_location=\"cpu\"));   v.eval()\n",
      "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_25992\\4085414792.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  e=EmotionNet(); e.load_state_dict(torch.load(\"models/emotion_model.pth\",map_location=\"cpu\")); e.eval()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Command: right\n",
      "ðŸš€ Command: right\n",
      "ðŸš€ Command: right\n",
      "ðŸš€ Command: right\n",
      "ðŸš€ Command: right\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m         cmd\u001b[38;5;241m=\u001b[39mACTIONS[voc]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸš€ Command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "g=GestureNet(); g.load_state_dict(torch.load(\"models/gesture_model.pth\",map_location=\"cpu\")); g.eval()\n",
    "v=VoiceNet();   v.load_state_dict(torch.load(\"models/voice_model.pth\",map_location=\"cpu\"));   v.eval()\n",
    "e=EmotionNet(); e.load_state_dict(torch.load(\"models/emotion_model.pth\",map_location=\"cpu\")); e.eval()\n",
    "\n",
    "ACTIONS=[\"stop\",\"up\",\"down\",\"left\",\"right\",\"forward\"]\n",
    "\n",
    "while True:\n",
    "    # Replace random data with live inputs from sensors/mic/camera\n",
    "    landmarks=np.random.rand(63).astype(np.float32)\n",
    "    mfcc=np.random.rand(40).astype(np.float32)\n",
    "    face=np.random.rand(1,48,48).astype(np.float32)\n",
    "\n",
    "    emo=torch.argmax(e(torch.tensor(face).unsqueeze(0))).item()\n",
    "    if emo==1:\n",
    "        cmd=\"land\"\n",
    "    else:\n",
    "        gest=torch.argmax(g(torch.tensor(landmarks).unsqueeze(0))).item()\n",
    "        if gest!=0:\n",
    "            cmd=ACTIONS[gest]\n",
    "        else:\n",
    "            voc=torch.argmax(v(torch.tensor(mfcc).unsqueeze(0))).item()\n",
    "            cmd=ACTIONS[voc]\n",
    "\n",
    "    print(f\"ðŸš€ Command: {cmd}\")\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3566fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ UDP socket ready (IP=127.0.0.1, PORT=5501)\n",
      " Voice model loaded.\n",
      " Emotion model loaded.\n",
      " Loaded gesture data: (600, 63) samples, 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\AppData\\Local\\Temp\\ipykernel_28884\\3190919987.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=\"cpu\"), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¹ Camera started successfully.\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ stop (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 1.0, 'forward': 0.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ down (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ DOWN\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 1.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ left (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ LEFT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=1.00) | {'down': 0.0, 'forward': 0.0, 'left': 0.0, 'right': 1.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=0.67) | {'down': 0.0, 'forward': 0.33, 'left': 0.0, 'right': 0.67, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ right (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ RIGHT\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.14, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Gesture â†’ forward (conf=1.00) | {'down': 0.0, 'forward': 1.0, 'left': 0.0, 'right': 0.0, 'stop': 0.0, 'up': 0.0}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [CAMERA] â†’ FORWARD\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Silence (RMS=0.00001)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.14, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00011)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00012)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00085)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00052)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.14, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00019)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00069)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00035)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00011)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00089)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Voice â†’ stop (conf=0.18) | [0.18 0.16 0.18 0.16 0.15 0.17]\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Silence (RMS=0.00021)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Silence (RMS=0.00007)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00017)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00014)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00043)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.15, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.15}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00079)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00012)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00007)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00037)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00065)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00009)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00037)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00026)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n",
      " Silence (RMS=0.00008)\n",
      " Final: [SYSTEM] â†’ HOVER\n",
      " Emotions â†’ {'angry': 0.15, 'disgust': 0.15, 'fear': 0.16, 'happy': 0.13, 'sad': 0.14, 'surprise': 0.13, 'neutral': 0.14}\n"
     ]
    }
   ],
   "source": [
    "import os, time, json, socket, warnings, collections\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import mediapipe as mp\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "UDP_IP, UDP_PORT = \"127.0.0.1\", 5501\n",
    "SAMPLE_RATE = 16000\n",
    "VOICE_N_MFCC = 40\n",
    "VAD_RMS_THRESH = 0.0015\n",
    "CYCLE_SECS = 1.0\n",
    "SURPRISE_THRESH = 0.15\n",
    "GESTURE_SMOOTH = 3\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "print(f\"ðŸ“¡ UDP socket ready (IP={UDP_IP}, PORT={UDP_PORT})\")\n",
    "\n",
    "\n",
    "def get_camera():\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\" Could not open webcam.\")\n",
    "    print(\"ðŸ“¹ Camera started successfully.\")\n",
    "    return cap\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "_mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=VOICE_N_MFCC,\n",
    "    melkwargs={\"n_fft\": 512, \"hop_length\": 160, \"n_mels\": VOICE_N_MFCC, \"center\": False}\n",
    ")\n",
    "\n",
    "def record_audio_get_mfcc(duration=1.0):\n",
    "    sd.default.reset()\n",
    "    audio = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    rms = float(np.sqrt(np.mean(np.square(audio))))\n",
    "    if rms < VAD_RMS_THRESH:\n",
    "        if DEBUG: print(f\" Silence (RMS={rms:.5f})\")\n",
    "        return None\n",
    "    waveform = torch.tensor(audio.T)\n",
    "    mfcc = _mfcc_transform(waveform).mean(dim=2)\n",
    "    mfcc = (mfcc - mfcc.mean()) / (mfcc.std() + 1e-6)\n",
    "    return mfcc\n",
    "\n",
    "class VoiceNet_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(1,8,3,padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(8,16,3,padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(320,64), nn.ReLU(),\n",
    "            nn.Linear(64,6)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        if len(x.shape)==2: x = x.unsqueeze(1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class EmotionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1,8,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(8,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*6*6,128), nn.ReLU(),\n",
    "            nn.Linear(128,7)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def safe_load(model, path, name):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(path, map_location=\"cpu\"), strict=False)\n",
    "        print(f\" {name} model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Could not load {name} model: {e}\")\n",
    "\n",
    "voice_model, emotion_model = VoiceNet_CNN(), EmotionNet()\n",
    "safe_load(voice_model, \"models/voice_model.pth\", \"Voice\")\n",
    "safe_load(emotion_model, \"models/emotion_model.pth\", \"Emotion\")\n",
    "voice_model.eval(); emotion_model.eval()\n",
    "\n",
    "ACTIONS = [\"stop\", \"up\", \"down\", \"left\", \"right\", \"forward\"]\n",
    "EMOTIONS = [\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
    "\n",
    "\n",
    "def load_gesture_data():\n",
    "    gesture_paths = {\n",
    "        \"up\": \"dataset/gesture_npy/up.npy\" if os.path.exists(\"dataset/gesture_npy/up.npy\") else \"/mnt/data/up.npy\",\n",
    "        \"down\": \"dataset/gesture_npy/down.npy\" if os.path.exists(\"dataset/gesture_npy/down.npy\") else \"/mnt/data/down.npy\",\n",
    "        \"left\": \"dataset/gesture_npy/left.npy\" if os.path.exists(\"dataset/gesture_npy/left.npy\") else \"/mnt/data/left.npy\",\n",
    "        \"right\": \"dataset/gesture_npy/right.npy\" if os.path.exists(\"dataset/gesture_npy/right.npy\") else \"/mnt/data/right.npy\",\n",
    "        \"forward\": \"dataset/gesture_npy/forward.npy\" if os.path.exists(\"dataset/gesture_npy/forward.npy\") else \"/mnt/data/forward.npy\",\n",
    "        \"stop\": \"dataset/gesture_npy/stop.npy\" if os.path.exists(\"dataset/gesture_npy/stop.npy\") else \"/mnt/data/stop.npy\",\n",
    "    }\n",
    "\n",
    "    X, y = [], []\n",
    "    for label, path in gesture_paths.items():\n",
    "        arr = np.load(path)\n",
    "        X.append(arr)\n",
    "        y += [label] * arr.shape[0]\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "    print(f\" Loaded gesture data: {X.shape} samples, {len(set(y))} classes.\")\n",
    "    return X, y\n",
    "\n",
    "X_gest, y_gest = load_gesture_data()\n",
    "gesture_clf = KNeighborsClassifier(n_neighbors=3)\n",
    "gesture_clf.fit(X_gest, y_gest)\n",
    "gesture_hist = collections.deque(maxlen=GESTURE_SMOOTH)\n",
    "\n",
    "\n",
    "def get_gesture_cmd(frame):\n",
    "    with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7) as hands:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "        if not res.multi_hand_landmarks:\n",
    "            gesture_hist.append(\"stop\")\n",
    "            return \"stop\", 0.0\n",
    "\n",
    "        lm = np.array([[lm.x, lm.y, lm.z] for lm in res.multi_hand_landmarks[0].landmark])\n",
    "        lm_min, lm_max = lm.min(axis=0), lm.max(axis=0)\n",
    "        norm = (lm - lm_min) / (lm_max - lm_min + 1e-6)\n",
    "        lm_flat = norm.flatten().reshape(1, -1)\n",
    "\n",
    "        cmd = gesture_clf.predict(lm_flat)[0]\n",
    "        probs = gesture_clf.predict_proba(lm_flat)[0]\n",
    "        conf = np.max(probs)\n",
    "        gesture_hist.append(cmd)\n",
    "        smooth = max(set(gesture_hist), key=gesture_hist.count)\n",
    "        if DEBUG: print(f\" Gesture â†’ {smooth} (conf={conf:.2f}) | {dict(zip(gesture_clf.classes_, np.round(probs,2)))}\")\n",
    "        return smooth, conf\n",
    "\n",
    "def get_voice_cmd(mfcc):\n",
    "    if mfcc is None: return None, 0.0\n",
    "    with torch.no_grad():\n",
    "        out = voice_model(mfcc)\n",
    "        probs = torch.softmax(out, dim=1).numpy().flatten()\n",
    "    cmd = ACTIONS[int(np.argmax(probs))]\n",
    "    conf = float(np.max(probs))\n",
    "    if DEBUG: print(f\" Voice â†’ {cmd} (conf={conf:.2f}) | {np.round(probs,2)}\")\n",
    "    return cmd, conf\n",
    "\n",
    "def get_emotion_cmd(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, 1.1, 5)\n",
    "    if len(faces) == 0:\n",
    "        return \"none\", 0.0\n",
    "    (x, y, w, h) = faces[0]\n",
    "    face = gray[y:y+h, x:x+w]\n",
    "    face = cv2.equalizeHist(face)\n",
    "    face = cv2.resize(face, (48, 48))\n",
    "    face_t = torch.tensor(face, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = emotion_model(face_t)\n",
    "        probs = torch.softmax(out, dim=1).numpy().flatten()\n",
    "\n",
    "    emotion = EMOTIONS[int(np.argmax(probs))]\n",
    "    conf = float(np.max(probs))\n",
    "\n",
    "    if DEBUG:\n",
    "        print(f\" Emotions â†’ {dict(zip(EMOTIONS, [round(p,2) for p in probs]))}\")\n",
    "\n",
    "    if emotion == \"surprise\" and conf >= SURPRISE_THRESH:\n",
    "        print(f\" Surprise confirmed (conf={conf:.2f}) â†’ LAND\")\n",
    "        return \"land\", conf\n",
    "\n",
    "    return emotion, conf\n",
    "\n",
    "# ======================================================\n",
    "#  MAIN LOOP\n",
    "# ======================================================\n",
    "def run():\n",
    "    cap = get_camera()\n",
    "    last_cycle = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # Emotion check every frame\n",
    "            emo_cmd, e_conf = get_emotion_cmd(frame)\n",
    "            if emo_cmd == \"land\":\n",
    "                payload = {\"source\": \"face\", \"command\": \"land\", \"timestamp\": time.strftime(\"%H:%M:%S\")}\n",
    "                sock.sendto(json.dumps(payload).encode(), (UDP_IP, UDP_PORT))\n",
    "                print(\" LAND triggered â†’ shutting down.\")\n",
    "                break\n",
    "\n",
    "            # Gesture & Voice every cycle\n",
    "            if time.time() - last_cycle > CYCLE_SECS:\n",
    "                last_cycle = time.time()\n",
    "\n",
    "                gesture_cmd, _ = get_gesture_cmd(frame)\n",
    "                mfcc = record_audio_get_mfcc(0.8)\n",
    "                voice_cmd, _ = get_voice_cmd(mfcc)\n",
    "                sd.default.reset()\n",
    "\n",
    "                # Priority: gesture > voice > hover\n",
    "                if gesture_cmd != \"stop\":\n",
    "                    final, src = gesture_cmd, \"camera\"\n",
    "                elif voice_cmd not in [None, \"stop\"]:\n",
    "                    final, src = voice_cmd, \"mic\"\n",
    "                else:\n",
    "                    final, src = \"hover\", \"system\"\n",
    "\n",
    "                payload = {\"source\": src, \"command\": final, \"timestamp\": time.strftime(\"%H:%M:%S\")}\n",
    "                sock.sendto(json.dumps(payload).encode(), (UDP_IP, UDP_PORT))\n",
    "                print(f\" Final: [{src.upper()}] â†’ {final.upper()}\")\n",
    "\n",
    "            # HUD overlay\n",
    "            overlay = frame.copy()\n",
    "            cv2.putText(overlay, f\"Emotion: {emo_cmd} ({e_conf:.2f})\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "            cv2.imshow(\"Drone Feed\", overlay)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "\n",
    "    finally:\n",
    "        cap.release(); cv2.destroyAllWindows(); sock.close()\n",
    "        print(\" Shutdown complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21835afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
